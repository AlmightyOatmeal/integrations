# Install:
#   This requires the python plugin. Ignore if you are using SignalFx installer. Otherwise, install it:
#
#     apt-get install collectd-python
#
#   Install the collectd plugin from github:
#
#     git clone https://github.com/signalfx/collectd-spark.git  /usr/share/collectd/collectd-spark
#
# Documentation:
#   https://github.com/signalfx/collectd-spark/blob/master/README.md
#
# System modifications:
#   None
#
# Config file modifications:
#   Change these settings to match your Spark installation.
#     - MetricsURL - URL (in form "http://host) for master or worker node if Metrics source (and therefore, by default Metrics HTTP Servlet Sink) are enabled
#     - MasterPort - Master (webui) port to query for metrics (defaulted in Spark to 8080) 
#     - WorkerPort - Worker (webui) port to query for metrics (defaulted in Spark Standalone to 8081)
#     - Applications - Set to "True" if you want to monitor applications in the cluster. Defaulted to "False"
#     - Master - URL for master application (in form "http://host:port")
#     - Cluster - Your Spark cluster mode ("Standalone", "Mesos" supported) - required if Applications set to "True"
#     - Dimension - include an additional dimension
#     - Dimensions - include multiple key-value pairs to dimension
#   Optionally, change any of the collectd settings according to your
#   requirements.

LoadPlugin python
<Plugin python>
  ModulePath "/opt/collectd-spark"

  Import spark_plugin

  <Module spark_plugin>
  MetricsURL "http://127.0.0.1"
  MasterPort 8080
  WorkerPort 8081
  Applications "True"
  Master "http://127.0.0.1:8080"
  Cluster "Standalone"
  Dimension "key=value"
  Dimensions "key1=value1,key2=value2,key3=value3"
  </Module>
</Plugin>
